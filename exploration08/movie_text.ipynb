{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ad69dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'feel', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n",
    "sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n",
    "\n",
    "# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n",
    "word_list = 'i feel hungry'.split()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad0f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"
     ]
    }
   ],
   "source": [
    "index_to_word={}  # 빈 딕셔너리를 만들어서\n",
    "\n",
    "# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \n",
    "# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \n",
    "index_to_word[0]='<PAD>'  # 패딩용 단어\n",
    "index_to_word[1]='<BOS>'  # 문장의 시작지점\n",
    "index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n",
    "index_to_word[3]='i'\n",
    "index_to_word[4]='feel'\n",
    "index_to_word[5]='hungry'\n",
    "index_to_word[6]='eat'\n",
    "index_to_word[7]='lunch'\n",
    "index_to_word[8]='now'\n",
    "index_to_word[9]='happy'\n",
    "\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "451bceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={word:index for index, word in index_to_word.items()}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37739333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ca65e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c3715e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \n",
    "encoded_sentences = get_encoded_sentences(sentences, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b410278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8d1e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314188be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 4 5 0]\n",
      " [1 3 6 7 0]\n",
      " [1 8 3 4 9]]\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "print(raw_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac87a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.03173524  0.00030694 -0.04715278  0.04862303]\n",
      "  [-0.01600902 -0.00762618 -0.03171761  0.02630046]\n",
      "  [-0.01899369 -0.04403085 -0.01871265 -0.00979627]\n",
      "  [-0.01914326  0.03272009  0.00346321  0.01773817]\n",
      "  [ 0.01745398  0.0177915   0.00745816  0.01211313]]\n",
      "\n",
      " [[-0.03173524  0.00030694 -0.04715278  0.04862303]\n",
      "  [-0.01600902 -0.00762618 -0.03171761  0.02630046]\n",
      "  [-0.01239384  0.00668408  0.01284531  0.03321261]\n",
      "  [ 0.01586861 -0.0357684  -0.02571403 -0.00870453]\n",
      "  [ 0.01745398  0.0177915   0.00745816  0.01211313]]\n",
      "\n",
      " [[-0.03173524  0.00030694 -0.04715278  0.04862303]\n",
      "  [ 0.02742702 -0.00589806  0.00563455  0.03069885]\n",
      "  [-0.01600902 -0.00762618 -0.03171761  0.02630046]\n",
      "  [-0.01899369 -0.04403085 -0.01871265 -0.00979627]\n",
      "  [ 0.03247798 -0.01923509  0.04410375 -0.04463902]]], shape=(3, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n",
    "word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n",
    "\n",
    "# tf.keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야 \n",
    "# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \n",
    "raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n",
    "raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=5)\n",
    "output = embedding(raw_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c14b1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 416       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 537\n",
      "Trainable params: 537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a92b6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          464       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,457\n",
      "Trainable params: 2,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(5))\n",
    "model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42b5bfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 4)           40        \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 89\n",
      "Trainable params: 89\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b36293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n",
      "17473536/17464789 [==============================] - 0s 0us/step\n",
      "훈련 샘플 개수: 25000, 테스트 개수: 25000\n"
     ]
    }
   ],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "# IMDb 데이터셋 다운로드 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09361809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨:  1\n",
      "1번째 리뷰 문장 길이:  218\n",
      "2번째 리뷰 문장 길이:  189\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])  # 1번째 리뷰데이터\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n",
    "print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이: ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "136041d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "1654784/1641221 [==============================] - 0s 0us/step\n",
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "print(index_to_word[1])     # 'the' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 1 이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a0d61a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
     ]
    }
   ],
   "source": [
    "# 보정 전 x_train[0] 데이터\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cfb910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n",
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다.\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])     # '<BOS>' 가 출력됩니다. \n",
    "print(word_to_index['the'])  # 4 이 출력됩니다. \n",
    "print(index_to_word[4])     # 'the' 가 출력됩니다.\n",
    "\n",
    "# 보정 후 x_train[0] 데이터\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6aa4162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "라벨:  1\n"
     ]
    }
   ],
   "source": [
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06683cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대 :  2494\n",
      "문장길이 표준편차 :  172.91149458735703\n",
      "pad_sequences maxlen :  580\n",
      "전체 문장의 0.94536%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e570007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15022eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1d1db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f16a644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 12s 66ms/step - loss: 0.6910 - accuracy: 0.5698 - val_loss: 0.6869 - val_accuracy: 0.6777\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 0.6797 - accuracy: 0.7469 - val_loss: 0.6718 - val_accuracy: 0.7660\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.6557 - accuracy: 0.7882 - val_loss: 0.6404 - val_accuracy: 0.7875\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.6128 - accuracy: 0.8147 - val_loss: 0.5932 - val_accuracy: 0.7953\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5532 - accuracy: 0.8315 - val_loss: 0.5358 - val_accuracy: 0.8101\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.4868 - accuracy: 0.8457 - val_loss: 0.4809 - val_accuracy: 0.8224\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4260 - accuracy: 0.8603 - val_loss: 0.4375 - val_accuracy: 0.8278\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3758 - accuracy: 0.8726 - val_loss: 0.4056 - val_accuracy: 0.8349\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3352 - accuracy: 0.8846 - val_loss: 0.3823 - val_accuracy: 0.8399\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3014 - accuracy: 0.8959 - val_loss: 0.3653 - val_accuracy: 0.8464\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2731 - accuracy: 0.9057 - val_loss: 0.3525 - val_accuracy: 0.8499\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2486 - accuracy: 0.9145 - val_loss: 0.3433 - val_accuracy: 0.8532\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2277 - accuracy: 0.9228 - val_loss: 0.3377 - val_accuracy: 0.8555\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2091 - accuracy: 0.9295 - val_loss: 0.3340 - val_accuracy: 0.8557\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1928 - accuracy: 0.9363 - val_loss: 0.3321 - val_accuracy: 0.8581\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1782 - accuracy: 0.9423 - val_loss: 0.3318 - val_accuracy: 0.8583\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1648 - accuracy: 0.9471 - val_loss: 0.3322 - val_accuracy: 0.8587\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1525 - accuracy: 0.9518 - val_loss: 0.3340 - val_accuracy: 0.8578\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1411 - accuracy: 0.9555 - val_loss: 0.3362 - val_accuracy: 0.8577\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1305 - accuracy: 0.9605 - val_loss: 0.3388 - val_accuracy: 0.8561\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2387755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 1s - loss: 0.3628 - accuracy: 0.8422\n",
      "[0.3628162443637848, 0.8422399759292603]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9fe51ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51f8ed9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
